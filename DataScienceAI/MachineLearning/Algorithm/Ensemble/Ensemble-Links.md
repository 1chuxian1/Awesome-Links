[![返回目录](https://user-images.githubusercontent.com/5803001/38079637-ff0abcf0-3371-11e8-9b76-ad651620afc7.jpg)](https://github.com/wxyyxc1992/Awesome-Links)

# Ensemble Learning Links

* [每个 Kaggle 冠军的获胜法门：揭秘 Python 中的模型集成](https://mp.weixin.qq.com/s/yY_-qJoza2xGRqrm40abkg)

- [AdaBoost--从原理到实现](http://blog.csdn.net/dark_scope/article/details/14103983)

- [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d?source=linkShare-fe48c4221a4c-1526129859): I am going to explain the pure vanilla version of the gradient boosting algorithm and will share links for its different variants at the end. 