[![返回目录](https://parg.co/UGo)](https://github.com/wxyyxc1992/Awesome-Reference) 
 
 


# 循环神经网络资料索引

* [2017-Unfolding RNNs #Series#](http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/): [RNN : Concepts and Architectures](http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/)

* [2015-RECURRENT NEURAL NETWORKS TUTORIAL](https://parg.co/bsS): Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many NLP tasks. But despite their recent popularity I’ve only found a limited number of resources that throughly explain how RNNs work, and how to implement them. That’s what this tutorial is about. It’s a multi-part series in which I’m planning to cover the following.

* [LSTM by Example using Tensorflow](https://parg.co/bsJ)

* [循环神经网络 RNN 打开手册](https://zhuanlan.zhihu.com/p/22930328)

* [RNN 以及 LSTM 的介绍和公式梳理 ](http://blog.csdn.net/Dark_Scope/article/details/47056361)

* [Awesome Recurrent Neural Networks](https://github.com/kjw0612/awesome-rnn):Github 上关于 RNN 的资源集锦

* [Anyone Can Learn To Code an LSTM-RNN in Python](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/): 简单的以讲解代码为主的 RNN 与 LSTM 入门教程

* [Recurrent Neural Networks for Beginners](https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82#.2fl2af7wa)

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

- [char-rnn #Project#](https://github.com/karpathy/char-rnn)

# LSTM

* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/): 中文版为 [理解 LSTM 网络](http://www.tuicool.com/articles/FjUjaeu)

* [LSTM NEURAL NETWORK FOR TIME SERIES PREDICTION](http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction)
